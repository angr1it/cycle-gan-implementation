{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb  3 08:50:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.04              Driver Version: 546.17       CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     On  | 00000000:01:00.0  On |                  N/A |\n",
      "| 30%   47C    P0              48W / 200W |    675MiB /  8192MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        25      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        36      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        37      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from training.datasets.loader import get_loader\n",
    "from training.models.model import CycleGAN\n",
    "from training.train import train\n",
    "from training.utils.logger import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "loader = get_loader('./datasets/monet2photo/', 'train', IMG_SIZE, BATCH_SIZE)\n",
    "\n",
    "cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using CUDA\" if cuda else \"Using CPU :(\")\n",
    "\n",
    "model = CycleGAN(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/080 [0786/0786] -- loss_G: 8.3108 | loss_G_identity: 2.2775 | loss_G_GAN: 0.9827 | loss_G_cycle: 5.0505 | loss_D: 0.2981 -- ETA: 11:31:50.46458760\n",
      "Epoch 002/080 [0786/0786] -- loss_G: 7.0050 | loss_G_identity: 1.9290 | loss_G_GAN: 0.9574 | loss_G_cycle: 4.1185 | loss_D: 0.2521 -- ETA: 11:21:52.372749\n",
      "Epoch 003/080 [0786/0786] -- loss_G: 6.6738 | loss_G_identity: 1.8475 | loss_G_GAN: 0.9613 | loss_G_cycle: 3.8651 | loss_D: 0.2194 -- ETA: 11:11:08.713593\n",
      "Epoch 004/080 [0786/0786] -- loss_G: 6.3718 | loss_G_identity: 1.7520 | loss_G_GAN: 0.9614 | loss_G_cycle: 3.6584 | loss_D: 0.1965 -- ETA: 10:58:38.149102\n",
      "Epoch 005/080 [0786/0786] -- loss_G: 6.1308 | loss_G_identity: 1.6778 | loss_G_GAN: 0.9189 | loss_G_cycle: 3.5341 | loss_D: 0.1773 -- ETA: 10:50:34.377748\n",
      "Epoch 006/080 [0786/0786] -- loss_G: 5.9239 | loss_G_identity: 1.6213 | loss_G_GAN: 0.8989 | loss_G_cycle: 3.4036 | loss_D: 0.1588 -- ETA: 10:41:29.392441\n",
      "Epoch 007/080 [0786/0786] -- loss_G: 5.6316 | loss_G_identity: 1.5506 | loss_G_GAN: 0.8481 | loss_G_cycle: 3.2329 | loss_D: 0.1484 -- ETA: 10:30:56.323545\n",
      "Epoch 008/080 [0786/0786] -- loss_G: 5.4604 | loss_G_identity: 1.4957 | loss_G_GAN: 0.8235 | loss_G_cycle: 3.1412 | loss_D: 0.1332 -- ETA: 10:20:53.327196\n",
      "Epoch 009/080 [0786/0786] -- loss_G: 5.3569 | loss_G_identity: 1.4645 | loss_G_GAN: 0.8044 | loss_G_cycle: 3.0880 | loss_D: 0.1267 -- ETA: 10:11:04.677866\n",
      "Epoch 010/080 [0786/0786] -- loss_G: 5.2038 | loss_G_identity: 1.4167 | loss_G_GAN: 0.8115 | loss_G_cycle: 2.9757 | loss_D: 0.1116 -- ETA: 10:01:36.207678\n",
      "Epoch 011/080 [0786/0786] -- loss_G: 5.2395 | loss_G_identity: 1.3964 | loss_G_GAN: 0.8750 | loss_G_cycle: 2.9680 | loss_D: 0.1059 -- ETA: 9:53:36.6911996\n",
      "Epoch 012/080 [0786/0786] -- loss_G: 5.2979 | loss_G_identity: 1.3804 | loss_G_GAN: 0.9280 | loss_G_cycle: 2.9895 | loss_D: 0.0950 -- ETA: 9:45:18.476317loss_G: 5.3234 | loss_G_identity: 1.3859 | loss_G_GAN: 0.9338 | loss_G_cycle: 3.0037 | loss_D: 0.0973 -- ETA: 9:47:11.106943\n",
      "Epoch 013/080 [0786/0786] -- loss_G: 4.9199 | loss_G_identity: 1.2984 | loss_G_GAN: 0.8568 | loss_G_cycle: 2.7647 | loss_D: 0.0960 -- ETA: 9:36:01.352917\n",
      "Epoch 014/080 [0786/0786] -- loss_G: 4.7793 | loss_G_identity: 1.2647 | loss_G_GAN: 0.8406 | loss_G_cycle: 2.6740 | loss_D: 0.0878 -- ETA: 9:26:54.079388\n",
      "Epoch 015/080 [0786/0786] -- loss_G: 5.2627 | loss_G_identity: 1.2900 | loss_G_GAN: 1.0809 | loss_G_cycle: 2.8917 | loss_D: 0.0901 -- ETA: 9:17:56.647030\n",
      "Epoch 016/080 [0786/0786] -- loss_G: 4.9113 | loss_G_identity: 1.2524 | loss_G_GAN: 0.9444 | loss_G_cycle: 2.7145 | loss_D: 0.0723 -- ETA: 9:08:58.204775\n",
      "Epoch 017/080 [0786/0786] -- loss_G: 4.6402 | loss_G_identity: 1.2020 | loss_G_GAN: 0.8939 | loss_G_cycle: 2.5443 | loss_D: 0.0712 -- ETA: 8:59:59.764468\n",
      "Epoch 018/080 [0786/0786] -- loss_G: 4.5204 | loss_G_identity: 1.1772 | loss_G_GAN: 0.8736 | loss_G_cycle: 2.4695 | loss_D: 0.0712 -- ETA: 8:51:06.717902\n",
      "Epoch 019/080 [0786/0786] -- loss_G: 4.6545 | loss_G_identity: 1.1895 | loss_G_GAN: 0.9101 | loss_G_cycle: 2.5549 | loss_D: 0.0650 -- ETA: 8:42:13.050308\n",
      "Epoch 020/080 [0786/0786] -- loss_G: 5.0282 | loss_G_identity: 1.1996 | loss_G_GAN: 1.0895 | loss_G_cycle: 2.7392 | loss_D: 0.0552 -- ETA: 8:33:28.951937\n",
      "Epoch 021/080 [0786/0786] -- loss_G: 4.5956 | loss_G_identity: 1.1428 | loss_G_GAN: 0.9885 | loss_G_cycle: 2.4643 | loss_D: 0.0602 -- ETA: 8:24:45.330598\n",
      "Epoch 022/080 [0786/0786] -- loss_G: 4.4353 | loss_G_identity: 1.1257 | loss_G_GAN: 0.9255 | loss_G_cycle: 2.3841 | loss_D: 0.0601 -- ETA: 8:15:58.604157\n",
      "Epoch 023/080 [0786/0786] -- loss_G: 4.2782 | loss_G_identity: 1.1042 | loss_G_GAN: 0.8847 | loss_G_cycle: 2.2893 | loss_D: 0.0550 -- ETA: 8:07:12.526552\n",
      "Epoch 024/080 [0786/0786] -- loss_G: 4.3794 | loss_G_identity: 1.1056 | loss_G_GAN: 0.9399 | loss_G_cycle: 2.3339 | loss_D: 0.0526 -- ETA: 7:58:29.284330\n",
      "Epoch 025/080 [0786/0786] -- loss_G: 4.4065 | loss_G_identity: 1.0978 | loss_G_GAN: 0.9612 | loss_G_cycle: 2.3475 | loss_D: 0.0512 -- ETA: 7:49:44.585287\n",
      "Epoch 026/080 [0786/0786] -- loss_G: 4.5545 | loss_G_identity: 1.1122 | loss_G_GAN: 1.0100 | loss_G_cycle: 2.4324 | loss_D: 0.0443 -- ETA: 7:41:03.573089\n",
      "Epoch 027/080 [0786/0786] -- loss_G: 4.2057 | loss_G_identity: 1.0679 | loss_G_GAN: 0.9208 | loss_G_cycle: 2.2171 | loss_D: 0.0489 -- ETA: 7:32:21.172470\n",
      "Epoch 028/080 [0786/0786] -- loss_G: 4.4621 | loss_G_identity: 1.0791 | loss_G_GAN: 1.0086 | loss_G_cycle: 2.3743 | loss_D: 0.0486 -- ETA: 7:23:43.734102\n",
      "Epoch 029/080 [0786/0786] -- loss_G: 4.2427 | loss_G_identity: 1.0337 | loss_G_GAN: 0.9495 | loss_G_cycle: 2.2595 | loss_D: 0.0376 -- ETA: 7:15:08.753455\n",
      "Epoch 030/080 [0786/0786] -- loss_G: 4.1054 | loss_G_identity: 1.0220 | loss_G_GAN: 0.9178 | loss_G_cycle: 2.1656 | loss_D: 0.0385 -- ETA: 7:06:32.353772\n",
      "Epoch 031/080 [0786/0786] -- loss_G: 4.0760 | loss_G_identity: 1.0312 | loss_G_GAN: 0.9129 | loss_G_cycle: 2.1319 | loss_D: 0.0408 -- ETA: 6:57:53.646956\n",
      "Epoch 032/080 [0786/0786] -- loss_G: 4.2390 | loss_G_identity: 1.0516 | loss_G_GAN: 0.9655 | loss_G_cycle: 2.2218 | loss_D: 0.0448 -- ETA: 6:49:16.206418\n",
      "Epoch 033/080 [0786/0786] -- loss_G: 4.0381 | loss_G_identity: 1.0154 | loss_G_GAN: 0.9218 | loss_G_cycle: 2.1009 | loss_D: 0.0377 -- ETA: 6:40:39.180840\n",
      "Epoch 034/080 [0786/0786] -- loss_G: 4.0525 | loss_G_identity: 1.0247 | loss_G_GAN: 0.9159 | loss_G_cycle: 2.1119 | loss_D: 0.0391 -- ETA: 6:32:04.115340\n",
      "Epoch 035/080 [0786/0786] -- loss_G: 4.2640 | loss_G_identity: 1.0561 | loss_G_GAN: 0.9902 | loss_G_cycle: 2.2177 | loss_D: 0.0331 -- ETA: 6:23:40.941239\n",
      "Epoch 036/080 [0786/0786] -- loss_G: 3.9169 | loss_G_identity: 0.9869 | loss_G_GAN: 0.9193 | loss_G_cycle: 2.0108 | loss_D: 0.0416 -- ETA: 6:15:15.710748\n",
      "Epoch 037/080 [0786/0786] -- loss_G: 3.9310 | loss_G_identity: 0.9796 | loss_G_GAN: 0.9479 | loss_G_cycle: 2.0034 | loss_D: 0.0293 -- ETA: 6:06:54.803791\n",
      "Epoch 038/080 [0786/0786] -- loss_G: 3.8882 | loss_G_identity: 0.9836 | loss_G_GAN: 0.9201 | loss_G_cycle: 1.9844 | loss_D: 0.0328 -- ETA: 5:58:24.453780\n",
      "Epoch 039/080 [0786/0786] -- loss_G: 3.8647 | loss_G_identity: 0.9846 | loss_G_GAN: 0.9129 | loss_G_cycle: 1.9672 | loss_D: 0.0284 -- ETA: 5:49:53.665567\n",
      "Epoch 040/080 [0347/0786] -- loss_G: 3.8660 | loss_G_identity: 0.9605 | loss_G_GAN: 0.9671 | loss_G_cycle: 1.9384 | loss_D: 0.0330 -- ETA: 5:46:07.086880"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/code/src/train.py:29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, device, save_path, epoch, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m model\u001b[38;5;241m.\u001b[39moptimizer_G\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     28\u001b[0m losses, fake_A, fake_B \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_losses_generator(real_A, real_B)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mlosses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss_G\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39moptimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39moptimizer_D_A\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, loader, cuda, epochs=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't add load for logger, so don't have full graphs before this point.\n",
    "\n",
    "Decided not to start training all over again.\n",
    "\n",
    "<img src=\"../assets/visdom_train_1.png\" alt=\"From visdom\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find any losses in ./output/losses\n"
     ]
    }
   ],
   "source": [
    "model = CycleGAN(cuda, lr=0.00005, load=True, load_dir='./output')\n",
    "logger = Logger(batches_epoch=len(loader), epoch=40, n_epochs=45)\n",
    "logger.load_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040/045 [0786/0786] -- loss_G: 3.4351 | loss_G_identity: 0.9141 | loss_G_GAN: 0.9699 | loss_G_cycle: 1.5511 | loss_D: 0.0040 -- ETA: 0:43:21.962811\n",
      "Epoch 041/045 [0786/0786] -- loss_G: 3.4751 | loss_G_identity: 0.9131 | loss_G_GAN: 0.9978 | loss_G_cycle: 1.5643 | loss_D: 0.0045 -- ETA: 0:34:36.054792\n",
      "Epoch 042/045 [0012/0786] -- loss_G: 3.4265 | loss_G_identity: 0.9238 | loss_G_GAN: 0.9374 | loss_G_cycle: 1.5653 | loss_D: 0.0057 -- ETA: 0:34:29.372508\n",
      "Stopped by user.\n"
     ]
    }
   ],
   "source": [
    "logger = train(model, loader, cuda, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042/050 [0786/0786] -- loss_G: 3.3808 | loss_G_identity: 0.9052 | loss_G_GAN: 0.9707 | loss_G_cycle: 1.5048 | loss_D: 0.0022 -- ETA: 1:09:00.224941\n",
      "Epoch 043/050 [0786/0786] -- loss_G: 3.3761 | loss_G_identity: 0.9042 | loss_G_GAN: 0.9788 | loss_G_cycle: 1.4931 | loss_D: 0.0015 -- ETA: 1:00:09.025191\n",
      "Epoch 044/050 [0786/0786] -- loss_G: 3.3761 | loss_G_identity: 0.9019 | loss_G_GAN: 0.9837 | loss_G_cycle: 1.4906 | loss_D: 0.0013 -- ETA: 0:51:19.410509\n",
      "Epoch 045/050 [0786/0786] -- loss_G: 3.3731 | loss_G_identity: 0.8994 | loss_G_GAN: 0.9843 | loss_G_cycle: 1.4894 | loss_D: 0.0012 -- ETA: 0:42:41.715641\n",
      "Epoch 046/050 [0786/0786] -- loss_G: 3.3641 | loss_G_identity: 0.8999 | loss_G_GAN: 0.9829 | loss_G_cycle: 1.4813 | loss_D: 0.0011 -- ETA: 0:34:08.651832\n",
      "Epoch 047/050 [0481/0786] -- loss_G: 3.3564 | loss_G_identity: 0.8980 | loss_G_GAN: 0.9817 | loss_G_cycle: 1.4767 | loss_D: 0.0011 -- ETA: 0:28:53.844236\n",
      "Stopped by user.\n"
     ]
    }
   ],
   "source": [
    "model = CycleGAN(cuda, lr=0.00002, load=True, load_dir='./output')\n",
    "logger = Logger(batches_epoch=len(loader), epoch=42, n_epochs=50)\n",
    "logger.load_losses()\n",
    "logger = train(model, loader, cuda, logger=logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
